\chapter{Background}
Add an opening comment here to talk a bit about what we will be presenting in this chapter.

\section{Computer Based Tests}
CBT abbreviation
- offers advantages such as being able to display higher quality visuals such as pictures, videos, graphs, etc...
- low paperwork, everything is stored on the computer
- automatic grading, less work for teachers
- generation of statistics is made easier by the fact that the data can be processed by the computer
- nowadays a lot of learning is done on computer systems, and children are used to dealing with computers so assessment through this medium is advantageous. \newline

CBTs are typically "fixed-item" tests where all the students answer the same set of questions,  usually provided by the person responsible for the assessment. This isn't ideal since students can be presented with questions that are too easy or too difficult for them to answer. Consequently, the results of the test won't give a very accurate representation of a student's ability, and for this reason, these types of tests aren't extremely useful. This problem lead to research and the development of computerized adaptive testing (CAT).

\section{Computerized Adaptive Testing}
\label{sec:CAT}
Computerized adaptive testing (CAT), also called \textit{tailored testing}, is a form of computer-based testing which administers questions (referred to as \textit{items} in the psychometrics literature) of the appropriate difficulty by adapting to the examinee's ability.
For example, if an examinee answers an item correctly, then the next item presented will higher on the difficulty scale. On the other hand, if they answer incorrectly, they will be presented with an item lower on the difficulty scale. \newline

From an architectural perspective, a computerized adaptive test (CAT) consists of five components \cite{CAT-Framework}:

\subsubsection{1. Calibrated item pool}
An item pool is needed to store all the items available for inclusion in a test. This item pool must be calibrated with a psychometric model. During this phase, the item parameters are estimated according to the chosen model and scaled to fit with already existing items. Usually, the psychometric model employed in these systems is called Item Response Theory (IRT) (section \ref{subsec:IRT}). Calibration is a complex process, and to be done accurately it requires a considerable amount of data. Typically, it is performed by psychometricians, aided by expensive and sophisticated calibration software.

\subsubsection{2. Starting point}
Initially, when zero items have been administered, no information is known about the examinees and so the CAT is unable to estimate their ability. As a result, the item selection algorithm will fail to choose the next item to be administered.
If there is previous information available, for example an examinee's ability estimate in a closely related subject, then this can be input into the system to form the starting point configuration. Often this data isn't available or too costly to collect, so the CAT's initial ability estimate for the examinee corresponds to the mean on the ability scale - hence the first item presented will be of average difficulty.

\subsubsection{3. Item selection algorithm}
The item selection algorithm chooses the next item to present to the examinee based on the ability estimate of the examinee up to that point. Several methods exist and largely depend on the psychometric model in use. One of the most commonly used methods is the \textit{maximum information method} (section \ref{subsec:IRT}), which selects the item which maximizes the information function with respect to the estimated ability at that point.

\subsubsection{4. Scoring algorithm}
The scoring algorithm refers to the steps taken to update the examinee's ability estimate after an item has been answered. The two most commonly used methods are \textit{maximum likelihood estimation} (section \ref{subsec:IRT}) and \textit{Bayesian estimation} (section \ref{subsec:IRT}).

\subsubsection{5. Termination criterion}
The termination criterion specifies when the CAT should finish. For example the CAT can terminate when the change in the ability estimate after each iteration is below a certain threshold, or when time has run out, or when $N$ items have been administered, etc... Obviously, the CAT shouldn't be terminated too early, so as to allow enough time to estimate the examinee's ability with acceptable accuracy.

\begin{figure}[H]
\centering
\includegraphics[scale=1]{cat_flowchart}
\caption{Flowchart of an adaptive test. Adapted from \cite{SIETTE}.}
\label{fig:cat_flowchart}
\end{figure}

The flowchart in figure~\ref{fig:cat_flowchart} corresponds to components 2-5, and illustrates the basics of the algorithm implemented in CAT. \cite{CAT-Wiki} gives a more detailed description of the procedure:
\begin{itemize}
\item[1.] The pool of items that haven't been administered yet is searched to determine the best item to present to the examinee, according to the current estimation of his ability.
\item[2.] The chosen item is presented to the examinee, who then answers it correctly or incorrectly.
\item[3.] The ability estimate is updated, based upon this new piece of information and the previous ability estimate.
\item[4.] Steps 1â€“3 are repeated until a termination criterion is met.
\item[5.] The algorithm returns a final ability estimate for the examinee's performance along with a confidence level: a percentage value indicating how accurate the estimate is.
\end{itemize}

CATs offer several advantages over traditional CBTs. As a result CATs have been used in many areas\cite{CAT-Areas}, such as education, job hiring, counselling, clinical studies, etc... Since CATs administer items by adapting to the examinee's ability, the test-taking experience ends up being a more positive one. Indeed, examinees won't have to deal with answering items which are too difficult or too easy compared to their ability level, a problem which appears in traditional CBTs.\newline

In addition, by administering only those items which will yield additional information, CATs end up being more accurate in estimating an examinee's ability level. This contrasts with CBTs which usually provide the best precision for examinees of medium ability, whereas extreme scores end up being less accurate.\newline

Lastly, CATs can come up with an ability estimate much quicker and with fewer administered items when compared to traditional CBTs. Indeed, an adaptive test can typically be shortened by 50\% and still maintain a higher level of precision than a fixed version.\cite{Weiss1984}
\newline

Despite the advantages mentioned above, CATs have some limitations. A frequent complaint is that an examinee isn't allowed to go back and change his answer to a past item. This limitation exists to prevent the examinee from intentionally answering items incorrectly to make subsequent items easier, and then going back and selecting the correct answers to achieve a perfect score. For similar reasons, it isn't possible to skip items, the examinee must select an answer to move on to the next item.\newline

The second issue has to do with the items themselves. First of all, there is the need for a large bank of items to cater to all ability levels. Developing an item pool of sufficient size can be very time consuming. David J. Weiss writes in \cite{Weiss1985} that item pools with 150-200 items are to be preferred, although 100 high quality items can sometimes be enough to achieve adequate estimations of ability levels. \newline

Secondly, for the CAT to be of good quality the item pool needs to be calibrated accurately. This requires pre-administering the items to a sizeable sample and then simultaneously estimating all the item parameters for each item. The guidelines in \cite{CAT-Primer} suggest that sample sizes may be as large as $1000$ examinees. This phase is costly, time consuming and often times simply unfeasible.\newline

Lastly, item exposure is a possible security concern. Sometimes particular items may be presented too often and become overused. This may result in examinees becoming familiar with them and sharing them to other examinees of the same ability level, thus corrupting the results of the test. This problem can be solved to some extent by modifying the item selection algorithm to include some exposure control mechanism.\newline

A brief overview of CATs was given in this section. All of these concepts will be explored in more detail in item response theory (section \ref{subsec:IRT}) and in the implementation of adaptive testing in jSCAPE (section 5.??).

\section{Probabilistic Test Theory}
\label{sec:probabilistic-test-theory}
In the previous section we listed some of the components necessary for the development of CATs. Many of these components, especially the item selection and scoring algorithms, rely heavily on probabilistic concepts. Therefore, in this section we go over a few topics in probability and how they can be implemented in a psychometric model to be used in computerized adaptive testing.

\subsection{Probability Theory}
\label{subsec:probability-theory}
Probability theory provides us with a means to model uncertainty in data and to infer information from observed data. This is especially useful when it comes to estimating latent variables, i.e. variables that are not directly observed, instead they are inferred from other observed variables. For instance, examinee ability is a latent variable, hence the reason for section \ref{sec:probabilistic-test-theory}.
\newline

The probability of an event E occurring is a numerical value between 0 and 1, indicating how probable it is that we will observe event E. It is denoted as $P(E)$ and $0 < P(E) < 1$. The value 1 indicates total certainty, whereas the value 0 indicates impossibility. \newline

In addition, there is the concept of conditional probability where the probability of an event $E_1$ occurring, given that event $E_2$ has occurred, can be denoted as $P(E_1|E_2)$. This concept is also important in statistical inference because it allows us to update prior beliefs given additional observed data. This is explained in more detail in sections \ref{subsec:likelihood-and-mle} and \ref{subsec:bayesian-inference}.

\subsection{Likelihood and Maximum Likelihood Estimation}
\label{subsec:likelihood-and-mle}
Although the terms probability and likelihood are used interchangeably in every day life, in statistics a distinction can be made.\newline

For any stochastic process, let us denote the observed outcomes as $x$ and the set of parameters as $\theta$. When we say probability, we want to calculate $P(x | \theta)$, i.e. the probability of observing the outcomes $x$ given specific values for the set of parameters $\theta$. \newline

However, sometimes we do not know the specific values for $\theta$, instead, we have observed some outcomes $x$, and want to find out how likely a particular value of $\theta$ is given the observed outcomes $x$. We call this the likelihood or likelihood function, and it is denoted as $L(\theta | x)$. The likelihood of a set of parameter values, $\theta$, given outcomes $x$, is equal to the probability of those observed outcomes given those parameter values, i.e. $L(\theta | x) = P(x | \theta)$.\newline

To highlight the distinction we illustrate with an example of how the two terms are used. If we consider a dice, a possible parameter is the fairness of the dice, while possible outcomes are which values are displayed after a roll. For instance, if a fair dice is rolled 5 times, what is the \textit{probability} that a 6 will show up on every roll? If a dice is rolled 5 times and lands on 6 every roll, what is the \textit{likelihood} that the dice is fair? \newline

\textit{Maximum likelihood estimation} refers to a method of statistical inference where one can find the set of parameter values ($\theta$) which are most \textit{likely} given the observed outcomes ($x$). As mentioned in the name of the method, this is done by finding the parameter values which maximize the likelihood function $L(\theta | x)$.

\subsection{Bayesian inference}
\label{subsec:bayesian-inference}
Bayesian inference is a method of statistical inference where Bayes' theorem is used to update the probability estimate for a hypothesis after observing additional events or outcomes.

\begin{equation} \label{eq:bayes-theorem}
P(A|B) = \dfrac{P(B|A)P(A)}{P(B)}
\end{equation}

Let us consider a small example: There is a 60\% chance of it snowing on Thursday. If it snows on Thursday, there is a 20\% chance of it snowing on Friday. If it didn't snow on Thursday, there is a 70\% chance it will snow on Friday.\newline

Before observing any additional data, we can say that the probability of it snowing on Thursday is 60\%. But, we are given additional information, namely that it snowed on Friday. How we can we update the probability that it snowed on Thursday to reflect this observation? \newline

We can do this by using Bayes' theorem shown in equation \eqref{eq:bayes-theorem}. Let $A$ be the event ``snowing on Thursday" and $B$ be the event ``snowing on Friday". Then we have:

$$P(A|B) = \dfrac{0.2 \times 0.6}{0.2 \times 0.6 + 0.7 \times 0.4} = 0.3$$

This example shows how additional observed data can affect one's prior beliefs. The observation that it snowed on Friday reduced the probability that it snowed on Thursday from 60\% to 30\%.
\newline

However, in the context of this project, we are interested in Bayesian inference of the probability distributions of parameters.

$$P(\theta|x) = \dfrac{P(x|\theta)P(\theta)}{P(x)}$$

The prior distribution, $P(\theta)$, is the distribution of the parameters $\theta$ before any data is observed. $P(x|\theta)$ is the distribution of the observed data conditioned on the parameters. The posterior distribution, $P(\theta|x)$, is the probability distribution of the parameters after considering the new information brought by the observed data.

\subsection{Item Response Theory}
\label{subsec:IRT}
We mentioned in section \ref{sec:CAT} that Item Response Theory (IRT) is usually the psychometric model of choice when developing a CAT, e.g. the Graduate Record Examination (GRE) and Graduate Management Admission Test (GMAT). CATs can still be implemented with Classical Test Theory but they offer less sophistication and less information to evaluate/improve the reliability of the test, making IRT the superior choice. \newline

In psychometrics, an item is a generic term used to refer to a question or an exercise. For instance, in a mathematics exam, "What is the square root of 81?" is a possible item. \newline

Item Response Theory gets its name from focusing on analyzing the items themselves  as opposed to Classical Test Theory, which considers the test as a whole. Indeed, Classical Test Theory judges an examinee's ability simply on the number of correct answers obtained, totally disregarding which items were answered correctly or incorrectly, thereby making the assumption that all items possess identical properties. \newline

IRT hinges on the idea that it is possible to model, as a mathematical function, the probability of a certain response to an item given the item parameters and the examinee's ability level. IRT is based on a set of strong assumptions\cite{IRT-Wiki}:

\begin{itemize}
\item[1.] A unidimensional trait denoted by $\theta$;
\item[2.] Local independence of items;
\item[3.] The response of a person to an item can be modelled by a mathematical \textit{item response function} (IRF).
\end{itemize}

The unidimensionality of the trait means that the items are supposed to measure one characteristic of the person, generally their ability, and that this trait should account for most of the variance in the test score. The trait level is denoted as the Greek letter theta ($\theta$), and typically it has a mean of 0 and a standard deviation of 1. For example, in the context of this project, theta ($\theta$) will represent an examinee's ability level in a particular programming subject (e.g. arrays, binary trees, etc...). Ability is a latent variable (section \ref{subsec:probability-theory}) because it isn't directly observable, therefore it must be inferred from the observable, concrete data such as examinee responses. \newline

The local independence of items states that an examinee's responses to items are independent of one another. This property of conditional independence is crucial to estimating examinee trait levels as we will see later on in this section.\newline

Several IRT models have been developed over the years to address the different types of tests that exist, e.g. multiple choice exams, agreement questionnaires (Likert scale), etc... These models all seek to achieve the same goal, modelling the examinee's ability on some ability scale, but they differ in the number of parameters associated with each item. We shall note that we only consider unidimensional IRT models which deal with dichotomously scored items, i.e. scored as correct or incorrect, in a multiple choice question for instance. We now take a look at these different models in more detail.

\subsubsection{The one-parameter logistic model}
Every IRT model is defined by two elements: a set of item parameters, and a mathematical item response function. This function plots the probability that an examinee of a given ability will answer a particular item correctly. The one-parameter logistic (1PL) model is the simplest IRT model because in this model items are only characterized by one parameter: the difficulty parameter $b_i$, where the subscript $i$ identifies item $i$. This has the effect of making the item response function quite simple.

\begin{equation} \label{eq:1PL-IRF}
P_i(\theta) = \dfrac{1}{1 + \me^{-(\theta - b_i)}}
\end{equation}

Equation \eqref{eq:1PL-IRF} shows the item response function for the 1PL model. $\theta$ is the examinee's ability level, $P_i(\theta)$ is the probability of answering item $i$ at all ability levels, and as mentioned above, $b_i$ is the item difficulty.

\begin{figure}[H]
\centering
\includegraphics[scale=1]{item_response_1pl}
\caption{The item response function of the 1PL model. (Source:\cite{Visual-IRT}).}
\label{fig:item_response_1pl}
\end{figure}

Figure \ref{fig:item_response_1pl} shows the probability of answering an item correctly for all ability levels. In the 1PL model, the item difficulty is the point at which a correct response and an incorrect response are equally probable. In the figure this is shown in red, and in this case, the item has difficulty parameter $b_i=1$. This highlights one of IRT's attractive properties, the fact that examinee ability and item difficulty are both measured on the same scale. The ability scale is a design choice, in this case it ranges from -4 to +4.

\begin{table}[ht] 
\centering
\begin{tabular}{c c c c c c c c c c}
\hline
Ability ($\theta$) & -4 & -3 & -2 & -1 & 0 & 1 & 2 & 3 & 4 \\
$P_1(\theta)$ & 0.007 & 0.018 & 0.047 & 0.119 & 0.269 & 0.5 & 0.731 & 0.881 & 0.953 \\
$P_2(\theta)$ & 0.047 & 0.119 & 0.269 & 0.5 & 0.731 & 0.881 & 0.953 & 0.982 & 0.993  \\
\hline
\end{tabular} 
\caption{Probabilities of a correct answer for two items in the 1PL model.}
\label{table:1PL_table}
\end{table}

Table \ref{table:1PL_table} shows the probability values for two 1PL items. $P_1(\theta)$ corresponds to the probability of answering item 1 (the item in figure \ref{fig:item_response_1pl}) correctly. $P_2(\theta)$ corresponds to the probability of answering item 2 (with difficulty $b_2=-1$) correctly. It can be seen that as the ability level of the examinee increases, so does the probability of him answering the item correctly. The difficulty parameter shifts the item response function to the left or to the right depending on its value. \newline

Although the 1PL model provides a good basis for IRT it is rather simplistic and fairly limited. For instance, the model assumes that at low ability levels the examinee has close to no chance to answer the item correctly. However, in multiple choice questions there is always the option of guessing randomly. These existing limitations bring us to considering the next model.

\subsubsection{The two-parameter logistic model}
The two-parameter logistic (2PL) model builds upon the 1PL model by adding a new item parameter: the discrimination ($a_i$), where the subscript $i$ identifies item $i$. Discrimination refers to an item's capacity to distinguish between examinees of different ability levels. Graphically, the discrimination parameter corresponds to the slope of the item response function, and typically it ranges from -2.8 to 2.8.

\begin{equation} \label{eq:2PL-IRF}
P_i(\theta) = \frac{1}{1 + \me^{-1.7a_i(\theta - b_i)}}
\end{equation}

Equation \eqref{eq:2PL-IRF} shows the item response function for the 2PL model. It is very similar to the equation of the 1PL model (Equation \eqref{eq:1PL-IRF}), except for the inclusion of the discrimination parameter ($a_i$) and a constant of $1.7$ to control the shape of the curve.

\begin{figure}[H]
\centering
\includegraphics[scale=1]{item_response_2pl}
\caption{The item response functions of three items in the 2PL model. (Source:\cite{Visual-IRT}).}
\label{fig:item_response_2pl}
\end{figure}

Figure \ref{fig:item_response_2pl} shows the item response functions of three items in the 2PL model. As in the 1PL model, the item difficulty is still located where $P_i(\theta)=0.5$, shown in the red lines. The item represented by the black curve and the item represented by the green curve both have the same discrimination, but different difficulty. As mentioned before, all this does is shift the item response function to the right or to the left, depending on which item you take as the reference point. \newline

The black and blue curves illustrate the other scenario. The item represented by the black curve and the item represented by the blue curve have different discrimination, but identical difficulty. This has the effect of making the blue curve much steeper than the black one, thus affecting the probabilities over the ability scale. Indeed, the item represented by the blue curve discriminates quite well between respondents. Examinees of low ability have a much lower probability of answering the item correctly than examinees of higher ability. Compare this to the item represented by the black curve, where examinees of low ability still have a fair chance of getting the item correct.

\subsubsection{The three-parameter logistic model}
The three-parameter logistic (3PL) model takes into account the possibility of guessing a correct response to an item. This model is especially convenient for tests where multiple choice questions are present. In IRT, this parameter is called the pseudo-guessing or chance parameter, and it is denoted as $c_i$, where the subscript $i$ identifies item $i$. Graphically, it corresponds to a lower asymptote, i.e. $P_i(-\infty)=c_i$. The guessing parameter does not vary as a function of the examinee's ability. Indeed, whether they be of low ability or high ability, examinees both have the same probability of guessing the correct answer to an item.

\begin{equation} \label{eq:3PL-IRF}
P_i(\theta) = c_i + (1-c_i) \frac{1}{1 + \me^{-1.7a_i(\theta - b_i)}}
\end{equation}

Equation \eqref{eq:3PL-IRF} shows the item response function for the three-parameter logistic model (3PL). The addition of $c_i$ defines a lower asymptote at that value, whereas $1-c_i$ acts as a weighting factor towards the 2PL model equation (Equation \eqref{eq:2PL-IRF}). Typically the pseudo-chance parameter ranges from 0 to 0.35, greater values being considered unacceptable. Lastly, if $c_i=0$ then we are simply left with a normal 2PL model.

\begin{figure}[H]
\centering
\includegraphics[scale=1]{item_response_3pl}
\caption{The item response function of the 3PL model. (Source:\cite{Visual-IRT}).}
\label{fig:item_response_3pl}
\end{figure}

Figure \ref{fig:item_response_3pl} shows the item response function of an item in the 3PL model. This item has parameters $a=1.4$, $b=0$ and $c=0.2$. Now it can be seen that the guessing phenomenon is taken into account by this IRT model. Indeed, at the lowest possible ability ($\theta=-4$), the probability of a correct answer is $P(-4)=0.200059$, which is very close to the guessing parameter of the item.\newline

Something to note is that $P_i(b_i) \neq 0.5$, i.e. the probability of a correct response at the ability level equal to the item difficulty is no longer 0.5. Instead, we have $P_i(b_i)= c + \dfrac{1-c}{2}$, and in figure \ref{fig:item_response_3pl} we have $P(0)=0.6$. If we consider a multiple choice question, adapted to the examinee's ability level, then it makes sense that the probability of them getting it correct would be greater than 0.5, to account for the possibility of getting it right merely through guessing.

\subsubsection{Item information}
In section \ref{sec:CAT}, when discussing item selection algorithms, we mentioned the maximum information method as a possible implementation. This method relies on the IRT concept of item information.\newline

In psychometrics and statistics, the term \textit{information} is defined as the reciprocal of the precision with which a parameter can be estimated\cite{Basics-IRT}. Therefore, item information is the precision in the ability estimate that the item provides, at all ability levels. It is also an indication of the quality of the item in terms of how well that item can discriminate between several respondents. \newline

The item information function for the 1PL model is calculated as
$$I(\theta) = P_i(\theta)Q_i(\theta),$$

where $Q_i(\theta) = 1-P_i(\theta)$.

\begin{figure}[H]
\centering
\includegraphics[scale=1]{item_information_1pl}
\caption{Item response function and item information
function for the 1PL model. (Source:\cite{Visual-IRT}).}
\label{fig:item_information_1pl}
\end{figure}

In figure \ref{fig:item_information_1pl}, two curves are plotted. The black curve is the item response function of an item, and the blue curve is the associated information function for that item. In the 1PL model, the maximum value of item information occurs where the probability of a correct answer and the probability of an incorrect answer are both equal to $0.5$. This point is indicated by the red line. If we recall earlier, this point also represents the difficulty of the item, i.e. item parameter $b_i$. Thus, the item provides the most information for those examinees whose ability is equal to the difficulty of the item, 1 in this case. This means that administering this item will give more precise ability estimates for examinees whose true ability is at that particular level. Presenting this item to examinees not at that particular level, examinees of ability -2 for instance, will not yield very much precision to subsequent ability estimates. \newline

The item information function for the 2PL model is calculated as
$$I(\theta) = a_i^2 P_i(\theta)Q_i(\theta),$$

where $Q_i(\theta) = 1-P_i(\theta)$. In the 2PL and 3PL models, the discrimination parameter, $a_i$, plays a significant role in the function, as it appears squared in the function.

\begin{figure}[H]
\centering
\includegraphics[scale=1]{item_information_2pl}
\caption{Item response functions and item information
functions for three items in the 2PL model. (Source:\cite{Visual-IRT}).}
\label{fig:item_information_2pl}
\end{figure}

Figure \ref{fig:item_information_2pl} shows three 2PL item response functions, in dotted lines, matched in color with the corresponding item information functions in solid lines. Like in the 1PL model, items still reach their maximum information at the point where ability is equal to the difficulty of the item. However, the amount of information provided at that ability level can now vary depending on how large the discrimination parameter is.\newline

The items represented by the black and blue curves both have the same difficulty parameter. But, the item represented by the blue curve has a higher discrimination parameter, and therefore this affects the item information function. Higher discrimination values lead to more item information, whereas lower discrimination values make the item less informative. \newline

The items represented by the black and green curves both have the same discrimination parameter, so the maximum information value will be the same for these items. However, these items do not share the same difficulty parameter. All this does is shift the curve along the ability axis, thus changing the location of maximum information for that item.\newline

The item information function for the 3PL model is calculated as
$$I(\theta) = a_i^2 \cdot \dfrac{(P_i(\theta)-c_i)^2}{(1-c_i)^2} \cdot \dfrac{Q_i(\theta)}{P_i(\theta)},$$

where $Q_i(\theta) = 1-P_i(\theta)$.

\begin{figure}[H]
\centering
\includegraphics[scale=1]{item_information_3pl}
\caption{Item response functions and item information
functions for two items in the 3PL model. (Source:\cite{Visual-IRT}).}
\label{fig:item_information_3pl}
\end{figure}

Figure \ref{fig:item_information_3pl} shows two 3PL item response functions, in dotted lines, matched in color with the corresponding item information functions in solid lines. In the 3PL model, the maximum of item information functions is no longer at the point where ability is equal to the difficulty of the item. Here, the guessing or pseudo-chance parameter ($c_i$) plays a role in the shape of the item information function. Higher guessing parameters lead to less item information, whereas lower guessing parameters lead to more item information. For instance, the item represented by the black curve has $c_i=0.1$, and the item represented by the red curve has $c_i=0.3$, which explains why the item represented by the black curve yields more information.

\subsubsection{Estimating the ability}
In section \ref{sec:CAT}, when discussing scoring algorithms, we saw that the two most common methods for ability estimation were \textit{maximum likelihood estimation} and \textit{Bayesian estimation}.\newline

In the \textit{maximum likelihood estimation} method, we need to define a likelihood function in terms of the ability level ($\theta$) we are trying to estimate:

\begin{equation*}
L(\theta | \textbf{u}) = L(\theta | u_1, ..., u_n) = \prod_{i=1}^n P_i(\theta)^{u_i}(1 - P_i(\theta))^{(1 - u_i)} ,
\end{equation*}

where $\textbf{u}=(u_1, ..., u_n)$ is called the response vector for an examinee, that is $u_i=1$ if the examinee answers the i$^\text{th}$ item correctly, and $u_i=0$ if the examinee answers the i$^\text{th}$ item incorrectly. $P_i(\theta)$ corresponds to the probability of answering the i$^\text{th}$ item correctly when the ability level is $\theta$, and thus $1 - P_i(\theta)$ gives the probability of answering the i$^\text{th}$ item incorrectly when the ability level is $\theta$. \newline

Now that the likelihood function is defined, we can apply maximum likelihood estimation to find the ability level which is most likely given the examinee's response vector. Let us illustrate with a very simplistic example where ability levels are discrete values between $-1$ and $+1$. We would iterate through these ability levels and compute the following values, for example:
\begin{align*}
L(\theta=-1 | \textbf{u}) &= 0.001 \\
L(\theta=+0 | \textbf{u}) &= 0.017 \\
L(\theta=+1 | \textbf{u}) &= 0.058
\end{align*}

The likelihood function is maximized when $\theta = +1$, and so this is the maximum likelihood estimate for $\theta$. In a CAT, and based on the examinee's response vector \textbf{u}, the system would assign $+1$ as the ability level for that examinee.\newline

In the \textit{Bayesian estimation} method, the CAT maintains a \textit{knowledge distribution} $P(\theta)$, which represents the probability that the examinee's ability is $\theta$. In the absence of any additional information, the CAT considers the examinee's initial knowledge distribution to be a uniform distribution.

\begin{figure}[H]
\centering
\includegraphics[scale=0.35]{uniform_distribution}
\caption{Initial examinee knowledge distribution.}
\label{fig:uniform_distribution}
\end{figure}

For example, figure \ref{fig:uniform_distribution} shows the initial examinee knowledge distribution in a CAT having discrete ability levels ranging from $-3$ to $3$. Continuous knowledge distributions are certainly possible as they allow for, in theory, infinitely more ability levels. However, they are more complex and require the use of integration to compute probabilities, thus for the purposes of illustration, we will only consider discrete knowledge distributions. \newline

The \textit{Bayesian estimation} method relies on Bayesian inference to derive the posterior knowledge distribution according to Bayes' rule:

$$P(\theta | \textbf{u}) = \dfrac{P(\textbf{u} | \theta)P(\theta)}{P(\textbf{u})} ,$$

where, again, \textbf{u} is the examinee's response vector and $\theta$ corresponds to the possible ability levels.\newline

The posterior distribution is the result of updating the prior knowledge distribution, $P(\theta)$, with observed data, $P(\textbf{u} | \theta)$, i.e. the examinee's response pattern. We can express this using the likelihood function:

$$P(\theta | \textbf{u}) \propto L(\theta | \textbf{u}) \cdot P(\theta)$$

After computing the posterior knowledge distribution, the CAT can estimate the ability estimation for the examinee. This is equal to the mode of the knowledge distribution, i.e. the ability associated with the highest probability value.

\begin{figure}[H]
\centering
\includegraphics[scale=0.35]{posterior_distribution}
\caption{Examinee knowledge distribution after answering an item correctly.}
\label{fig:posterior_distribution}
\end{figure}

For example, figure \ref{fig:posterior_distribution} shows the knowledge distribution of an examinee after being shown an item of medium difficulty and answering it correctly. It can be seen that lower ability levels have become less likely, whereas higher ability levels are now more probable.\newline

The ability level associated with the highest probability is $\theta=1$, thus this becomes the estimated ability. Over time, after the examinee answers more items, the estimation will become associated with even higher probability values. These probabilities indicate how confident the CAT is in that particular ability estimation.

\section{Summary}
In this section, the relevant background literature and theory for this project was discussed.\newline

Firstly, we discussed types of computer based assessment and how they differ from regular assessment techniques. We then looked at an improvement over traditional computer tests in the form of computerized adaptive testing (CAT), where the difficulty of the questions in the test is adapted to fit the student's ability. \newline

We provided an overview of the probability concepts necessary to understand Item Response Theory, and explained in quite some detail the techniques behind Item Response Theory. These will become the basis of one of the adaptive difficulty algorithms in jSCAPE. In the context of Item Response Theory, we have seen three models designed to calculate the probabilities of answering questions correctly, and knowledge estimation techniques in those particular models.\newline

In the next chapter we take a look at related work which will help us in developing jSCAPE. We focus on how these works approach the design of some of the components of such a system, i.e. providing exercises, exercise generation, collecting statistical data, adapting the difficulty, etc...