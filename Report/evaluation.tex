\chapter{Evaluation}
\label{chap:evaluation}
In this chapter we evaluate the strengths and weaknesses of jSCAPE and its various components and features, with respect to the objectives we set at the beginning of the project.

\section{Evaluating jSCAPE}
\subsubsection{Evaluating the graphical user interface}
When justifying our choices for technologies, we mentioned that one of the reasons for using Java applets and JavaFX was to provide a user-friendly and visually appealing user interface. Evidence\cite{Interface-study} suggests that students will prioritise a visually appealing user interface over powerful features. Therefore, we must make sure that our interface is generally well received amongst students, for jSCAPE to be actually useful.\newline

A small amount of data was gathered during an undergraduate fair on 02/06/2014, and during a meeting with student friends on 07/06/2014. In total we asked 10 students what they thought of the jSCAPE interface. The comments were mostly positive, however a few students mentioned that the interface was too large, requiring full screen at times. Two students also mentioned that we should be careful not to overload the view with information, particularly the Profile tab where all the statistics are displayed. We would need more data to identify possible improvements, if any, in the user interface. \newline

We didn't ask for feedback on the user interface of the admin tool as we feel this is less of a factor when it comes to whether teachers actually use the tool. Nonetheless, we think that the GUI of the admin tool is sufficiently user friendly for its purpose.

\subsubsection{Evaluating current jSCAPE exercises}
At the time of writing this report, we have implemented four exercise generators for the exercise categories of Binary Trees, Conditionals, Syntax and Strings. Some example exercises are shown in appendix \ref{chap:example-jscape-exercises}. \newline

For Binary Tree exercises, we supply a binary tree as exercise data, and ask the student to select what is printed after a specific traversal order. This type of exercise was intended to show that it was possible to bring variety to jSCAPE exercises. We think that this exercise is useful for learning the different traversal orders, however the exercise loses its usefulness very quickly once a student has mastered traversal orders. Perhaps this exercise would be more effective if we asked the student to type in the whole traversal instead of selecting the correct answer among the four different traversals.\newline

Next, for Conditionals and Strings exercises, we display a code snippet as exercise data, and ask the student to determine the final value of one or more variables. The usefulness of this type of exercise has been demonstrated in research. Indeed, \cite{Lister} has shown that this type of exercise provides effective assessment on a student's understanding of code semantics.\newline

Thanks to our random code generators, jSCAPE is able to provide many of such exercises, with very different code snippets. This means that a teacher could automatically generate 1000 Conditionals exercises and all of them would include very different code snippets, thus providing students with a large supply of exercises to self-assess their understanding of control flow in Java. However, sometimes the generated code can end up being too random, and the control flow doesn't resemble anything that would appear in a ``real" program. As such, more work would be needed in the code generating component, to make the generated code more useful in the learning process. \newline

Finally, Syntax exercises also present a code snippet, but this time containing syntax errors. The exercise asks the student to spot the syntax error, or to identify how many syntax errors the piece of code contains. We believe that this is a useful exercise for students to familiarise themselves with the Java language, especially in the early stages, when students are discovering programming languages for the first time. \newline

We would like to point out that teachers and educators have more knowledge of what constitutes a useful exercise in assessing students knowledge of a particular concept. These exercises were only intended to show what can be achieved in the jSCAPE system.

\subsubsection{Evaluating the jSCAPE exercise format}
The jSCAPE exercise format was sufficient for us to implement four exercise generators for multiple choice type questions. The format is quite flexible to allow for different views to be used in providing exercise data to accompany the question. The format was also designed with other exercise types in mind. For instance, if the exercise displayer were to read \textsf{FillInBlank} in the \textsf{$<$view$>$} tag of the second \textsf{$<$display$>$} tag, then it would ignore all the \textsf{$<$choice$>$} tags and simply create a text field for the student to type in his answer. \newline

However, in general we are not that satisfied with the exercise format. It isn't very elegant and we could see it posing some restrictions in the future. \newline

\begin{lstlisting}[language=xml, caption={A modified exercise format.}, label=lst:new_exercise_format]
<?xml version="1.0"?>
<exercise>
	<display>
		<view>........</view>
		<value>.......</value>
	</display>
	<display>
		<view>........</view>
		<value>.......</value>
	</display>
	<display>
		<view>........</view>
		<value>.......</value>
	</display>
</exercise>
\end{lstlisting}

We believe that it should be modified to allow for more exercise types to be added to jSCAPE. The listing in \ref{lst:new_exercise_format} shows a possible modification, which makes the exercise format more straightforward. Each possible \textsf{$<$view$>$} could have a component implemented in Java which takes as parameter the value of the \textsf{$<$value$>$} tag. This is how the \textsf{BinaryTree} and \textsf{CodeEditor} were implemented, but for some unknown reason, we didn't think about doing the same with other views such as \textsf{MultipleChoice}. This would offer a more general purpose exercise format than jSCAPE currently has at the moment.

\subsubsection{Evaluating automated exercise generation}
Exercises of the Binary Trees category were very suitable for automated generation. Indeed, generating random binary trees with a random number of nodes is something relatively easy to do, and the randomness of the binary tree doesn't impact the quality of the exercise.\newline

For exercises which display code snippets, we mentioned before that this generated code can sometimes be too random, and that ``strange" code can sometimes be produced. This can definitely impact the quality and effectiveness of the exercise. For instance, sometimes an \textsf{if} branch would contain the statement \textsf{var6=true} followed by the exact same statement on the next line. This kind of statement would never appear in a program written by a competent programmer, thus presenting this code snippet to a student could very well be counter productive. \newline

We believe that we have shown how exercise generators can be written to provide, theoretically, endless amounts of exercises. This was one of the objectives we set for the project at the beginning of development. However, it still remains that an exercise generator must be written manually for each new type of exercise. This isn't a very scalable solution, and thus we discuss some possible improvements for automated exercise generation, in the next chapter.

\subsubsection{Evaluating computerized adaptive testing in jSCAPE}
As part of adapting the difficulty of exercises to the student's ability, we implemented two exercise selection algorithms, one based on difficulty categories and another one based on Item response theory.\newline

The difficulty category idea was inspired by PAT\cite{PAT} and we improved the algorithm to include it in jSCAPE. The improvement was based on evidence\cite{Abdullah} which suggests that at most three exercises of a given difficulty are needed to remove uncertainty from assessing a student's knowledge.\newline

The algorithm performed well, thanks to the state machine implementation it is correct in that it will always choose an exercise of the appropriate difficulty category. It is less sophisticated than the Item response theory based algorithm, however it is much easier for teachers, because all they have to do is assign the exercise to one of three categories. But the fact that there are only three exercise categories means that there is less distinction between the difficulty of exercises, as opposed to IRT where we had 11 difficulty levels and the discrimination parameter which also affected perceived difficulty.\newline

Difficulty categories could also be assigned through the automated exercise generation process. To do this we used simple metrics such as binary tree size, code length or number of variables. This worked well under the assumption that Binary Tree exercises become more difficult as they grow in size or as they become unbalanced, however we aren't sure this assumption is valid. It may be valid early on when students are learning about binary trees for the first time. \newline

Exercises with code snippets were much more suitable for automatically assigning to a difficulty category because more metrics were available. For Conditionals, we used the number of \textsf{if/else} branches and the number of variables asked for. This worked quite well, however the randomness of the code would still interfere sometimes. A more sophisticated code analysis would be needed to determine the difficulty category and we discuss some improvements in the next chapter.\newline

The more sophisticated exercise selection algorithm we implemented was based on Item response theory. We managed to test the correctness of the implemented algorithm by running a simulation, whose results are included in appendix \ref{chap:irt-simulation}. It shows that the algorithm does indeed choose the most appropriate exercise according to the student's estimated knowledge level. The knowledge distribution is also updated correctly.\newline

However, this solution did present some limitations, most notably, it requires teachers to input the item parameters, that is the difficulty and discrimination parameters. Although guidelines were given for this step, the results will be a lot less accurate than what could be achieved through using calibration software and data. Still, it provides a bit more options than the difficulty category based method. \newline

We also evaluate our system in terms of the advantages and disadvantages mentioned in CAT (section \ref{sec:CAT}). Most of the complaints for CATs are for when they are used as means to assess students for a grade which counts towards their end of year total. For instance, not being able to go back and change one's answer. In a self-assessment setting, where the results don't really count towards anything, this isn't much of a problem. However, jSCAPE does present the most common weakness of CATs, in that a large bank of calibrated exercises is needed to be effective. Finally, we mentioned that exercise exposure could be an issue in CATs, but in jSCAPE we aren't concerned about this, because the system is only intended for self-assessment.

\subsubsection{Evaluating collection of statistical data}
To determine which statistics should be collected by jSCAPE, we inspired ourselves from other software, in particular video games, with statistics such as win/losses, history over time, etc...In addition, we regularly received feedback from our supervisor, Dr. Tristan Allwood, in terms of which statistics would be useful for a teacher or lecturer in monitoring students' progress. Asking more lecturers or teachers could help in determining more useful statistics that jSCAPE could record and display to students or teachers. It would be quite easy to add more statistics, however we believe that the core useful statistics have been implemented.

\section{Summary}
In this chapter we evaluated the different components that form the jSCAPE system. We showed that collection and display of statistical data was implemented quite solidly, and that the system was able to support provision of exercises very well.\newline

However, jSCAPE did present certain limitations in its exercise format, in its exercise generators and in the adaptive difficulty component. \newline

In the next chapter we conclude the project and give some possibilities of future work which could address the weaknesses of the system discussed in this section.